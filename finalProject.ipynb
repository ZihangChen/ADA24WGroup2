{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8pI0mMsOKkz"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import scipy.stats as stats\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwUfN8SjPuC2",
        "outputId": "fc6d9698-e85c-4fa3-9538-7d53d0eb689c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ghs1bqffP7vt"
      },
      "outputs": [],
      "source": [
        "# from zipfile import ZipFile\n",
        "# with ZipFile('./drive/MyDrive/DevGPT.zip', 'r') as zipObj:\n",
        "#   zipObj.extractall('./drive/MyDrive/DevGPT')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6Irm28COKk0",
        "outputId": "8edaffa8-c0ca-458b-e2b1-35cd9589d31e"
      },
      "outputs": [],
      "source": [
        "# File list\n",
        "dir_list = []\n",
        "for dir in os.listdir('./drive/MyDrive/DevGPT/DevGPT'):\n",
        "    if 'snapshot' in dir:\n",
        "        dir_list.append(f'./drive/MyDrive/DevGPT/DevGPT/{dir}')\n",
        "\n",
        "print(dir_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRKS-88XUplc",
        "outputId": "c2575c05-82b5-4f47-e0ce-a21a9b767c06"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qEcBKdCOKk0"
      },
      "source": [
        "#### Loading Json File by Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGeplH37OKk1"
      },
      "outputs": [],
      "source": [
        "def load_json(file_path):\n",
        "    json_file_path = file_path\n",
        "\n",
        "    with open(json_file_path, 'r') as file:\n",
        "        # Load JSON data into a Python object\n",
        "        return json.load(file)\n",
        "\n",
        "def execute_in_all(filelist, func):\n",
        "    result_list = []\n",
        "    for folder in filelist:\n",
        "        for filename in os.listdir(folder):\n",
        "            if filename.endswith('.json'):\n",
        "                file_path = os.path.join(folder, filename)\n",
        "                data = load_json(file_path)\n",
        "                result_list += func(data)\n",
        "    return result_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EWcvqdEOKk1"
      },
      "source": [
        "### RQ1. What is the distribution of different issues that developer present to ChatGPT on link shared to github?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwZOcp8TOKk1"
      },
      "source": [
        "#### Method 1: Visualization of Different type of Github Links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R73tHZqKOKk1"
      },
      "outputs": [],
      "source": [
        "type_list = []\n",
        "\n",
        "def count_type(data):\n",
        "    type_list = []\n",
        "    for source in data['Sources']:\n",
        "        create_time = 'CreatedAt' if 'CreatedAt' in source else 'AuthorAt'\n",
        "        type_list.append((source['Type'], source[create_time]))\n",
        "    return type_list\n",
        "\n",
        "type_list += execute_in_all(filelist=dir_list, func=count_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_ZyJfVwOKk1",
        "outputId": "2c0b8656-21a1-4d29-80a7-f7d80ad5c2c4"
      },
      "outputs": [],
      "source": [
        "# Gather the # of each type\n",
        "type_dict = {}\n",
        "for type in type_list:\n",
        "    if type[0] not in type_dict:\n",
        "        type_dict[type[0]] = [1]\n",
        "    else:\n",
        "        type_dict[type[0]][0] += 1\n",
        "\n",
        "print(type_dict)\n",
        "\n",
        "time_dict = {}\n",
        "for type in type_list:\n",
        "    time = type[1][:7]\n",
        "    if time not in time_dict:\n",
        "        time_dict[time] = {}\n",
        "    if type[0] not in time_dict[time]:\n",
        "        time_dict[time][type[0]] = 1\n",
        "    else:\n",
        "        time_dict[time][type[0]] += 1\n",
        "\n",
        "print(time_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iv_Xs-pBOKk1"
      },
      "outputs": [],
      "source": [
        "type_df = pd.DataFrame.from_dict(type_dict)\n",
        "time_df = pd.DataFrame.from_dict(time_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-SKHnRKOKk1",
        "outputId": "46b7ef49-45ab-4d77-9bc3-012f8f426345"
      },
      "outputs": [],
      "source": [
        "time = time_df.sort_index(axis=1)\n",
        "print(time)\n",
        "\n",
        "row_sum = time.iloc[5].sum()\n",
        "print(row_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "8RI5KBxzOKk2",
        "outputId": "487044ae-f4dd-4ba2-8145-a4df6709e001"
      },
      "outputs": [],
      "source": [
        "type_df.plot(kind='bar', legend=True)\n",
        "plt.ylabel('Counts')\n",
        "plt.title('GPT Share Link')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "NGKZ7jXPOKk2",
        "outputId": "8ada54f7-c609-4ab3-8307-6ab23513e3da"
      },
      "outputs": [],
      "source": [
        "df = time_df\n",
        "\n",
        "df.columns = pd.to_datetime(df.columns)\n",
        "\n",
        "df = df.sort_index(axis=1)\n",
        "df = df.loc[:, df.columns.year >= 2022]\n",
        "\n",
        "# Plot scatter plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for category in df.index:\n",
        "    ax.scatter(df.columns, df.loc[category], label=category)\n",
        "\n",
        "# Add legend and labels\n",
        "ax.legend()\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Counts')\n",
        "ax.set_title('Counts of Categories Over Time')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvNjtmSOOKk2"
      },
      "outputs": [],
      "source": [
        "# Find data from 2022 Jan\n",
        "def find_2022(data):\n",
        "    data_list = []\n",
        "    for source in data['Sources']:\n",
        "        create_time = 'CreatedAt' if 'CreatedAt' in source else 'AuthorAt'\n",
        "        if source[create_time][:7] == '2022-01':\n",
        "            data_list.append(source)\n",
        "    return data_list\n",
        "\n",
        "share_2022 = execute_in_all(filelist=dir_list, func=find_2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OwApw_SOKk2",
        "outputId": "baae1ac6-b3fb-4f7c-92b8-62edad6d4e80"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "# Prints the nicely formatted dictionary\n",
        "pprint.pprint(share_2022[0])\n",
        "# Sets 'pretty_dict_str' to the formatted string value\n",
        "pretty_dict_str = pprint.pformat(share_2022[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_9FkXwBOKk2"
      },
      "source": [
        "##### Conclusion 1\n",
        "The user often respond to old post with ChatGPT links, this result in the inaccuracy of data.\n",
        "To Do: Use data of the ChatGPT link to estimatetime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcEbrhSyOKk2",
        "outputId": "00a4a778-5e0b-4fa0-f295-c3dab350095d"
      },
      "outputs": [],
      "source": [
        "type_list = []\n",
        "\n",
        "def count_type(data):\n",
        "    type_list = []\n",
        "    for source in data['Sources']:\n",
        "        for share in source['ChatgptSharing']:\n",
        "            if 'DateOfConversation' in share:\n",
        "                type_list.append((source['Type'], share['DateOfConversation']))\n",
        "    return type_list\n",
        "\n",
        "type_list += execute_in_all(filelist=dir_list, func=count_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_M2f1VOOKk2",
        "outputId": "b64f285d-f3c0-4f6e-bad9-b21046171b90"
      },
      "outputs": [],
      "source": [
        "time_dict = {}\n",
        "for type in type_list:\n",
        "    try:\n",
        "        date = pd.to_datetime(type[1], format=\"%B %d, %Y\")\n",
        "        time = date.strftime(\"%B %Y\")\n",
        "        if time not in time_dict:\n",
        "            time_dict[time] = {}\n",
        "        if type[0] not in time_dict[time]:\n",
        "            time_dict[time][type[0]] = 1\n",
        "        else:\n",
        "            time_dict[time][type[0]] += 1\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(time_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Belqz31OOKk2"
      },
      "outputs": [],
      "source": [
        "time_df = pd.DataFrame.from_dict(time_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "oJGkyYwqOKk2",
        "outputId": "5d0dab63-7445-493c-c1c6-1182e6fcce29"
      },
      "outputs": [],
      "source": [
        "df = time_df\n",
        "\n",
        "df.columns = pd.to_datetime(df.columns)\n",
        "\n",
        "df = df.sort_index(axis=1)\n",
        "# df = df.loc[:, df.columns.year >= 2022]\n",
        "\n",
        "category_colors = {\n",
        "    'hacker news': 'red',\n",
        "    'pull request': 'blue',\n",
        "    'issue': 'green',\n",
        "    'discussion': 'orange',\n",
        "    'commit': 'purple',\n",
        "    'code file': 'brown'\n",
        "}\n",
        "\n",
        "# Plot scatter plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for category in df.index:\n",
        "    color = category_colors.get(category, 'black')  # Default to black if color not defined\n",
        "    ax.scatter(df.columns, df.loc[category], label=category, color=color)\n",
        "\n",
        "    # Add lines connecting points\n",
        "    ax.plot(df.columns, df.loc[category], linestyle='-', alpha=0.5, color=color)\n",
        "\n",
        "# Add legend and labels\n",
        "ax.legend()\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Counts')\n",
        "ax.set_title('Counts of Categories Over Time')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqpWOBBwOKk2"
      },
      "source": [
        "##### Conclusion 2\n",
        "There is a significant amout of increase in june and july for chatgpt usage. However, the reduce after august might be affected by the fact that more data are collected during augest.\n",
        "Observation from the chart:\n",
        " - People are more instereted in disscussion and solving issues using ChatGpt in May and June.\n",
        " - There is a significant amount increase in pr involve ChatGPT link in July and August. (might suggest users are using chatgpt for coding or code reviewing).\n",
        "    - Suggesting higher confident in ChatGPT's ability in terms of coding.\n",
        " - There are high amount of usage in code file. This cover many different areas, e.g. ChatGPT related toolkits and demos, code initiated with chatgpt, ChatGPT assisted documentation, etc.\n",
        "    - This means the code file does not directly related to coding questions since majroity of it is documentation related.\n",
        " - There is little disscussion, this might because people dont tend to use github disscussion.\n",
        "    - In contrary the high amount in issue, showing ChatGPT is implemented for work related scenarios very quickly.\n",
        " - ChatGPT usage in PR might suggest different reason, e.g. coding style, request update, etc. Can be consider similar to issue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "DVfc0LoNOKk2",
        "outputId": "dc502915-ecc2-4d0e-a95e-79e3f4c9e1c7"
      },
      "outputs": [],
      "source": [
        "# Get counts for each category for the latest date\n",
        "category_counts = type_df.loc[0]\n",
        "\n",
        "# Plot pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(category_counts, labels=category_counts.index, colors=[category_colors.get(cat, 'black') for cat in category_counts.index], autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Distribution of Categories')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "YOXP2TA7OKk3",
        "outputId": "bdf8058b-391a-44ea-8c42-7eaf28c1eac5"
      },
      "outputs": [],
      "source": [
        "# Get counts for each category for the latest date\n",
        "new_df = pd.DataFrame.transpose(df)\n",
        "category_counts = new_df.loc['2023-05-01']\n",
        "\n",
        "# Plot pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(category_counts, labels=category_counts.index, colors=[category_colors.get(cat, 'black') for cat in category_counts.index], autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Distribution of Categories')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "n-FjnbE6OKk3",
        "outputId": "9df2911a-c34d-4edd-eeff-a70ec217a316"
      },
      "outputs": [],
      "source": [
        "# Get counts for each category for the latest date\n",
        "new_df = pd.DataFrame.transpose(df)\n",
        "category_counts = new_df.loc['2023-07-01']\n",
        "\n",
        "# Plot pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(category_counts, labels=category_counts.index, colors=[category_colors.get(cat, 'black') for cat in category_counts.index], autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Distribution of Categories')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "0hb7d69YOKk3",
        "outputId": "8a9de3d5-1067-4389-9a52-db947393ee3f"
      },
      "outputs": [],
      "source": [
        "# Get counts for each category for the latest date\n",
        "new_df = pd.DataFrame.transpose(df)\n",
        "category_counts = new_df.loc['2023-09-01']\n",
        "\n",
        "# Plot pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(category_counts, labels=category_counts.index, colors=[category_colors.get(cat, 'black') for cat in category_counts.index], autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Distribution of Categories')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSXtAGWdOKk3"
      },
      "source": [
        "##### Conclusion 3\n",
        "Observation from pie chart:\n",
        " - In May most ChatGPT usage are disscusion related.\n",
        " - In July there is significant increase in ChatGPT usage in directly coding related topics.\n",
        " - In September, the direct usage in pr reduced but increase in commit.\n",
        "     - This might suggest ChatGPT usage become less welcomed in code base with multiple maintainers.\n",
        "     - Whereas for smaller independent projects general coding, ChatGPT is considered as sufficient tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6zKPAZIOKk3",
        "outputId": "2783a6df-21dd-4d3e-dc4a-01b90de2c0fb"
      },
      "outputs": [],
      "source": [
        "correlation_matrix = new_df.corr()\n",
        "print(correlation_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVu2mT1NOKk3"
      },
      "source": [
        "#### Conclusion 4\n",
        "Base on correlation analysis:\n",
        " - hacker news: strong correlation with issues, disscussion. Consider they are all disscusion related.\n",
        " - code file: strong correlation to everything but commit. Referencing the nature of code file can be very diverse.\n",
        " - commit: low correlation with other categories. The usage of ChatGPT in commit often suggest direct practice of using ChatGPT for coding (referencing ChatGPT link in code).\n",
        "    - The low correlation with other types in comparison, might suggest most other type of usage of ChatGPT is disscussion related.\n",
        "    - The change in amount/percentage of commit might shows developer's overall confident level on using ChatGPT for coding assist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOmMEMe8OKk3"
      },
      "source": [
        "# RQ#2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHsxDo3hOKk3"
      },
      "source": [
        "1. here we want to find a way to tell what symbolizes satifaction with a response ie(no more prompting... a thank you)\n",
        "\n",
        "2. use that information to see what is said before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-rbBlKEOKk3"
      },
      "outputs": [],
      "source": [
        "\n",
        "#get all data\n",
        "\n",
        "def find_2022(data):\n",
        "    data_list = []\n",
        "    for source in data['Sources']:\n",
        "        data_list.append(source)\n",
        "    return data_list\n",
        "\n",
        "all_data = execute_in_all(filelist=dir_list, func=find_2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "9qsqaCyWOKk3",
        "outputId": "a9cfe5b6-d607-4860-cd65-88f67baca6a7"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(all_data[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k18YRvMmOKk3",
        "outputId": "fa3c830a-a09f-4c20-de3a-065a1c4e1325"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(all_data[1]['ChatgptSharing'][0]['Conversations'][0]['Answer']) #example of answer access\n",
        "\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJDs8fLYOKk3",
        "outputId": "a0bb4d97-329f-4856-ac83-3a5f86b45b21"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(all_data[1]['ChatgptSharing'][0]['Conversations'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxcc3VixOKk3"
      },
      "source": [
        "Note data is in order of when asked\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4NLuywvOKk3",
        "outputId": "5297ade9-2213-4403-b50d-1329a182bc93"
      },
      "outputs": [],
      "source": [
        "max_prompts = 0\n",
        "total_prompts = 0\n",
        "num_missing = 0\n",
        "num_entries = 0\n",
        "list_of_num_prompts = []\n",
        "\n",
        "for entry in all_data:\n",
        "    num_entries += 1\n",
        "\n",
        "    for sharing in entry['ChatgptSharing']:\n",
        "\n",
        "        # Check if 'NumberOfPrompts' key exists in the dictionary\n",
        "        if 'NumberOfPrompts' in sharing:\n",
        "            list_of_num_prompts.append(sharing['NumberOfPrompts'])\n",
        "            total_prompts += sharing['NumberOfPrompts']\n",
        "\n",
        "            if sharing['NumberOfPrompts'] > max_prompts:\n",
        "                max_prompts = sharing['NumberOfPrompts']\n",
        "\n",
        "        else:\n",
        "            #print(\"NumberOfPrompts key missing in this sharing entry\")\n",
        "            num_missing += 1\n",
        "            if 'Conversations' in sharing:\n",
        "                print(sharing['Conversations'])\n",
        "\n",
        "\n",
        "print(\"The maximum number of prompts is:\", max_prompts)\n",
        "print(\"The total number of prompts is:\", total_prompts)\n",
        "print(\"The average number of prompts is:\", total_prompts / len(all_data))\n",
        "print(\"The number of sharing entries missing the 'NumberOfPrompts' key is:\", num_missing)\n",
        "print(\"The number of sharing entries is:\", num_entries)\n",
        "print(\"the percentage of missing entries is:\", num_missing/num_entries) #percentage of missing entries\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rNY1g9bOKk3"
      },
      "source": [
        "Clean data by removing missing conversations and taking out ' and \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itQjc10ZOKk4",
        "outputId": "3d13f66f-84c2-4280-97e5-deca044f815e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cleaning text data by removing special characters and handling missing data\n",
        "for i, entry in tqdm(enumerate(all_data)):\n",
        "    for j, sharing in enumerate(entry['ChatgptSharing']):\n",
        "        new_conversations = []\n",
        "        if 'Conversations' not in sharing:\n",
        "            continue\n",
        "        for k, conversation in enumerate(sharing['Conversations']):\n",
        "            # Check if 'Prompt' and 'Answer' keys exist\n",
        "            if 'Prompt' in conversation and 'Answer' in conversation:\n",
        "                # Clean special characters\n",
        "                cleaned_prompt = conversation['Prompt'].replace('\\'', '').replace('\\\"', '')\n",
        "                cleaned_answer = conversation['Answer'].replace('\\\"', '').replace('\\'', '')\n",
        "                # Update the conversation with cleaned data\n",
        "                conversation['Prompt'] = cleaned_prompt\n",
        "                conversation['Answer'] = cleaned_answer\n",
        "            # Always append the possibly modified conversation\n",
        "            new_conversations.append(conversation)\n",
        "        # Update the Conversations list with cleaned conversations\n",
        "        all_data[i]['ChatgptSharing'][j]['Conversations'] = new_conversations\n",
        "\n",
        "\n",
        "\n",
        "pprint.pprint(all_data[1]) #example of cleaned data\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "TUaHVcxsOKk4",
        "outputId": "1640601c-faa2-4986-a801-d2676eaa24f6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Get the frequency of each number of prompts\n",
        "frequency = [list_of_num_prompts.count(i) for i in range(1, max_prompts+1)]\n",
        "\n",
        "# Create the x-axis values (number of prompts)\n",
        "x = list(range(1, max_prompts+1))\n",
        "\n",
        "# Plot the graph\n",
        "plt.bar(x, frequency)\n",
        "plt.xlabel('Number of Prompts')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Number of Prompts by Frequency')\n",
        "plt.xticks(np.arange(0, 100, step=5))\n",
        "plt.xlim(1, 100)\n",
        "plt.ylim(0, 5000)\n",
        "plt.show()\n",
        "# Create a DataFrame with the frequency of each number of prompts\n",
        "df_prompts = pd.DataFrame({'Frequency': list_of_num_prompts})\n",
        "\n",
        "# Get the statistics of the number of prompts\n",
        "statistics = df_prompts.describe()\n",
        "\n",
        "print(statistics)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "conclusion 1 majority of the data falls within 5 prompts and therefore is likely resolved in 5 prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT9AqRBWOKk4"
      },
      "source": [
        "I want to gert the avg over time for sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFe3UF9jV1K3",
        "outputId": "b2df3b5a-32c6-45a4-8abc-991cf91e2061"
      },
      "outputs": [],
      "source": [
        "%pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0dHl-qfOKk7",
        "outputId": "278e9f29-0061-4c21-e4c2-fe565ccf2cbf"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "# Assuming all_data is already loaded with data like the provided example\n",
        "conversations = {}\n",
        "\n",
        "\n",
        "# Assuming all_data is already loaded with data like the provided example\n",
        "from collections import defaultdict\n",
        "\n",
        "# Initialize a dictionary to hold lists of prompts and answers by their index in the conversation\n",
        "conversations_by_index = defaultdict(list)\n",
        "\n",
        "# Correctly iterating over the 'ChatgptSharing' array and then 'Conversations'\n",
        "for entry in tqdm(all_data):\n",
        "    if 'ChatgptSharing' in entry:\n",
        "        for sharing in entry['ChatgptSharing']:\n",
        "            if 'Conversations' in sharing:\n",
        "                # Iterate through each conversation\n",
        "                for idx, conv in enumerate(sharing['Conversations']):\n",
        "                    # Append the prompt and answer to the appropriate list in the dictionary\n",
        "                    conversations_by_index[idx].append((conv['Prompt'], conv['Answer']))\n",
        "\n",
        "# Now 'conversations_by_index' is a dictionary where each key is an index and the value is a list of tuples\n",
        "# Each tuple is (Prompt, Answer) from all conversations at that position\n",
        "\n",
        "# Assuming 'conversations_by_index' is filled as described, where each index maps to a list of (Prompt, Answer) tuples\n",
        "\n",
        "# Initialize TF-IDF vectorizers for prompts and answers\n",
        "prompt_vectorizer = TfidfVectorizer()\n",
        "answer_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Dictionaries to store TF-IDF matrices for prompts and answers separately\n",
        "prompt_tfidf_matrices = {}\n",
        "answer_tfidf_matrices = {}\n",
        "\n",
        "# Dictionaries to store cosine similarity results for prompts and answers separately\n",
        "prompt_similarity_averages = {}\n",
        "answer_similarity_averages = {}\n",
        "\n",
        "for index, conv_group in tqdm(conversations_by_index.items()):\n",
        "    # Extract separate lists for prompts and answers\n",
        "    prompts = [pair[0] for pair in conv_group]\n",
        "    answers = [pair[1] for pair in conv_group]\n",
        "\n",
        "    # Compute TF-IDF vectors for prompts and answers separately\n",
        "    if prompts:\n",
        "        prompt_tfidf_matrix = prompt_vectorizer.fit_transform(prompts)\n",
        "        prompt_tfidf_matrices[index] = prompt_tfidf_matrix\n",
        "\n",
        "        # Compute cosine similarity for prompts\n",
        "        if prompt_tfidf_matrix.shape[0] > 1:\n",
        "            prompt_similarity_matrix = cosine_similarity(prompt_tfidf_matrix)\n",
        "            np.fill_diagonal(prompt_similarity_matrix, 0)\n",
        "            prompt_avg_similarity = np.sum(prompt_similarity_matrix) / (prompt_similarity_matrix.shape[0] * (prompt_similarity_matrix.shape[0] - 1))\n",
        "            prompt_similarity_averages[index] = prompt_avg_similarity\n",
        "\n",
        "    if answers:\n",
        "        answer_tfidf_matrix = answer_vectorizer.fit_transform(answers)\n",
        "        answer_tfidf_matrices[index] = answer_tfidf_matrix\n",
        "\n",
        "        # Compute cosine similarity for answers\n",
        "        if answer_tfidf_matrix.shape[0] > 1:\n",
        "            answer_similarity_matrix = cosine_similarity(answer_tfidf_matrix)\n",
        "            np.fill_diagonal(answer_similarity_matrix, 0)\n",
        "            answer_avg_similarity = np.sum(answer_similarity_matrix) / (answer_similarity_matrix.shape[0] * (answer_similarity_matrix.shape[0] - 1))\n",
        "            answer_similarity_averages[index] = answer_avg_similarity\n",
        "\n",
        "# Output average similarity results for each index for prompts and answers\n",
        "print(\"Average Cosine Similarity Scores by Prompt Index:\")\n",
        "for idx, avg_sim in prompt_similarity_averages.items():\n",
        "    print(f\"Prompt Index {idx}: {avg_sim:.4f}\")\n",
        "print(\"Average Cosine Similarity Scores by Answer Index:\")\n",
        "for idx, avg_sim in answer_similarity_averages.items():\n",
        "    print(f\"Answer Index {idx}: {avg_sim:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbKqFZFEOKk8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Assuming prompt_similarity_averages and answer_similarity_averages are dictionaries filled as previously described\n",
        "\n",
        "# Lists to hold the indices and corresponding average similarities\n",
        "prompt_indices = list(prompt_similarity_averages.keys())\n",
        "prompt_similarities = [prompt_similarity_averages[idx] for idx in prompt_indices]\n",
        "\n",
        "answer_indices = list(answer_similarity_averages.keys())\n",
        "answer_similarities = [answer_similarity_averages[idx] for idx in answer_indices]\n",
        "\n",
        "# Plotting the average cosine similarity scores for prompts\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(prompt_indices, prompt_similarities, marker='o', linestyle='-', color='blue', label='Prompts')\n",
        "plt.plot(answer_indices, answer_similarities, marker='s', linestyle='--', color='red', label='Answers')\n",
        "plt.xlabel('Prompt/Answer Index')\n",
        "plt.ylabel('Average Cosine Similarity')\n",
        "plt.title('Average Cosine Similarity Scores by Index for Prompts and Answers (one to all)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "conclusion 2: the correlation of one to all rises with the number of prompts until it hits a large drop and then rises with number of prompts until the end. This could be since developers are asking smilar questions to the previous the further they go in conversation. The big drop is likely due to a switch in the topic of prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofwXFDdDOKk8"
      },
      "source": [
        "sentiment analysis... try to do this but compsre the sentiment for each response number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pwcxTWSOKk8"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "from collections import defaultdict\n",
        "\n",
        "def analyze_sentiments(data):\n",
        "    # Assuming all_data is already loaded with data like the provided example\n",
        "    conversations_by_index = defaultdict(list)\n",
        "\n",
        "    # Correctly iterating over the 'ChatgptSharing' array and then 'Conversations'\n",
        "    for entry in tqdm(data):\n",
        "        if 'ChatgptSharing' in entry:\n",
        "            for sharing in entry['ChatgptSharing']:\n",
        "                if 'Conversations' in sharing:\n",
        "                    # Iterate through each conversation\n",
        "                    for idx, conv in enumerate(sharing['Conversations']):\n",
        "                        # Append the prompt, answer, and their sentiments to the appropriate list in the dictionary\n",
        "                        prompt_sentiment = TextBlob(conv['Prompt']).sentiment.polarity\n",
        "                        answer_sentiment = TextBlob(conv['Answer']).sentiment.polarity\n",
        "                        conversations_by_index[idx].append({\n",
        "                            'Prompt': conv['Prompt'],\n",
        "                            'Prompt Sentiment': prompt_sentiment,\n",
        "                            'Answer': conv['Answer'],\n",
        "                            'Answer Sentiment': answer_sentiment\n",
        "                        })\n",
        "\n",
        "    # Now 'conversations_by_index' is a dictionary where each key is an index and the value is a list of dictionaries\n",
        "    # Each dictionary contains the prompt, the answer, and their respective sentiments\n",
        "\n",
        "    # To calculate average sentiment per index:\n",
        "    average_sentiment_by_index = {}\n",
        "\n",
        "    for idx, conversations in conversations_by_index.items():\n",
        "        total_prompt_sentiment = sum(c['Prompt Sentiment'] for c in conversations)\n",
        "        total_answer_sentiment = sum(c['Answer Sentiment'] for c in conversations)\n",
        "        average_prompt_sentiment = total_prompt_sentiment / len(conversations)\n",
        "        average_answer_sentiment = total_answer_sentiment / len(conversations)\n",
        "        average_sentiment_by_index[idx] = {\n",
        "            'Average Prompt Sentiment': average_prompt_sentiment,\n",
        "            'Average Answer Sentiment': average_answer_sentiment\n",
        "        }\n",
        "\n",
        "    # # Print average sentiments for each index\n",
        "    # for idx, sentiments in average_sentiment_by_index.items():\n",
        "    #     print(f\"Index {idx}:\")\n",
        "    #     print(f\"  Average Prompt Sentiment: {sentiments['Average Prompt Sentiment']:.2f}\")\n",
        "    #     print(f\"  Average Answer Sentiment: {sentiments['Average Answer Sentiment']:.2f}\")\n",
        "    return average_sentiment_by_index\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSobVQF1OKk8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def graph_sentiments(average_sentiment, type_name):\n",
        "    # Lists to hold the indices and corresponding average sentiments\n",
        "    indices = list(average_sentiment.keys())\n",
        "    average_prompt_sentiments = [average_sentiment[idx]['Average Prompt Sentiment'] for idx in indices]\n",
        "    average_answer_sentiments = [average_sentiment[idx]['Average Answer Sentiment'] for idx in indices]\n",
        "\n",
        "    # Plotting the average sentiment scores for prompts\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(indices, average_prompt_sentiments, label='Average Prompt Sentiment', marker='o', linestyle='-', color='blue')\n",
        "    plt.xlabel('Conversation Index')\n",
        "    plt.ylabel('Average Sentiment Score')\n",
        "    plt.title(f'Average Sentiment Scores by Conversation Index for Prompts for {type_name}')  # Corrected title format\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting the average sentiment scores for answers\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(indices, average_answer_sentiments, label='Average Answer Sentiment', marker='s', linestyle='--', color='green')\n",
        "    plt.xlabel('Conversation Index')\n",
        "    plt.ylabel('Average Sentiment Score')\n",
        "    plt.title(f'Average Sentiment Scores by Conversation Index for Answers for {type_name}')  # Corrected title format\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7qiqV1gOKk8"
      },
      "outputs": [],
      "source": [
        "issue = []\n",
        "discussion = []\n",
        "commit = []\n",
        "code_file = []\n",
        "pull_request = []\n",
        "hacker_news = []\n",
        "\n",
        "\n",
        "for i, item in enumerate(all_data):\n",
        "    if item['Type'] == 'issue':\n",
        "        issue.append(item)\n",
        "    elif item['Type'] == 'discussion':\n",
        "        discussion.append(item)\n",
        "    elif item['Type'] == 'commit':\n",
        "        commit.append(item)\n",
        "    elif item['Type'] == 'code file':\n",
        "        code_file.append(item)\n",
        "    elif item['Type'] == 'pull request':\n",
        "        pull_request.append(item)\n",
        "    elif item['Type'] == 'hacker news':\n",
        "        hacker_news.append(item)\n",
        "\n",
        "\n",
        "types = [issue, discussion, commit, code_file, pull_request, hacker_news]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUFBaq04OKk8"
      },
      "outputs": [],
      "source": [
        "#run all sentiment analysis\n",
        "\n",
        "all_data_sentiment = analyze_sentiments(all_data)\n",
        "graph_sentiments(all_data_sentiment, 'All Data')\n",
        "issue_sentiment = analyze_sentiments(issue)\n",
        "graph_sentiments(issue_sentiment, 'Issue')\n",
        "discussion_sentiment = analyze_sentiments(discussion)\n",
        "graph_sentiments(discussion_sentiment, 'Discussion')\n",
        "commit_sentiment = analyze_sentiments(commit)\n",
        "graph_sentiments(commit_sentiment, 'Commit')\n",
        "code_file_sentiment = analyze_sentiments(code_file)\n",
        "graph_sentiments(code_file_sentiment, 'Code File')\n",
        "pull_request_sentiment = analyze_sentiments(pull_request)\n",
        "graph_sentiments(pull_request_sentiment, 'Pull Request')\n",
        "hacker_news_sentiment = analyze_sentiments(hacker_news)\n",
        "graph_sentiments(hacker_news_sentiment, 'Hacker News')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "zcvtrMVBlZVI",
        "outputId": "435185f0-1850-40c9-be93-d9692026d84a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# List to hold all texts for batch processing\n",
        "texts = []\n",
        "indices = []\n",
        "\n",
        "# Collect all texts\n",
        "for i, item in enumerate(all_data):\n",
        "    if 'ChatgptSharing' in item:\n",
        "        for j, sharing in enumerate(item['ChatgptSharing']):\n",
        "            if 'Conversations' in sharing:\n",
        "                for k, conversation in enumerate(sharing['Conversations']):\n",
        "                    if 'Prompt' in conversation and 'Answer' in conversation:\n",
        "                        indices.append((i, j, k))  # Store index to know where to put back embeddings\n",
        "                        texts.append(conversation['Prompt'])\n",
        "                        texts.append(conversation['Answer'])\n",
        "\n",
        "# Encode all texts at once in batches\n",
        "embeddings = model.encode(texts, convert_to_tensor=True, batch_size=32)  # Adjust batch size based on your system capabilities\n",
        "\n",
        "# Split embeddings back into individual tensors and update original data structure\n",
        "for idx, (i, j, k) in enumerate(indices):\n",
        "    prompt_idx = 2 * idx  # Even index\n",
        "    answer_idx = 2 * idx + 1  # Odd index\n",
        "    all_data[i]['ChatgptSharing'][j]['Conversations'][k]['Prompt Embedding'] = embeddings[prompt_idx].cpu().numpy()\n",
        "    all_data[i]['ChatgptSharing'][j]['Conversations'][k]['Answer Embedding'] = embeddings[answer_idx].cpu().numpy()\n",
        "\n",
        "# # Optionally, if you need to free up GPU memory:\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    # Calculate cosine similarity between current and previous prompt\n",
        "    if idx > 0:  # Ensure there is a previous prompt\n",
        "        previous_prompt_idx = 2 * (idx - 1)\n",
        "        similarity = cosine_similarity(embeddings[previous_prompt_idx].unsqueeze(0), embeddings[prompt_idx].unsqueeze(0))\n",
        "        similarities.append(similarity.item())\n",
        "    else:\n",
        "        similarities.append(0)  # No similarity for the first prompt\n",
        "\n",
        "# Plotting the similarities\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(similarities, marker='o', linestyle='-', color='b')\n",
        "plt.title('Cosine Similarity between Successive Prompts')\n",
        "plt.xlabel('Prompt Index')\n",
        "plt.ylabel('Cosine Similarity')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "TOkXEkXetJlD",
        "outputId": "99f79fa7-84d1-42ed-8779-0c795817c13d"
      },
      "outputs": [],
      "source": [
        "i, j = 0, 0  # Adjust these indices to point to the desired 'ChatgptSharing' and 'Conversations'\n",
        "\n",
        "# Extract the specific conversation data\n",
        "conversation_data = all_data[i]['ChatgptSharing'][j]['Conversations']\n",
        "similarities = []\n",
        "for k in range(1, len(conversation_data)):\n",
        "    # Convert NumPy arrays back to tensors\n",
        "    current_embedding = torch.tensor(conversation_data[k]['Prompt Embedding'])\n",
        "    previous_embedding = torch.tensor(conversation_data[k-1]['Prompt Embedding'])\n",
        "\n",
        "    # Calculate cosine similarity and append to list\n",
        "    similarity = cosine_similarity(current_embedding.unsqueeze(0), previous_embedding.unsqueeze(0))\n",
        "    similarities.append(similarity.item())\n",
        "\n",
        "# Insert 0 at the beginning since the first prompt has no previous prompt to compare with\n",
        "similarities.insert(0, 0)\n",
        "\n",
        "# Plotting the similarities\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(similarities, marker='o', linestyle='-', color='b')\n",
        "plt.title('Cosine Similarity between Successive Prompts in a Conversation')\n",
        "plt.xlabel('Prompt Index')\n",
        "plt.ylabel('Cosine Similarity')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion 3: taken all these artifacts together we can observe that we have to use a combination of correlation and sentiment anlysis to determine if the developer is satisfied. In general we would like to see a low correlation and a high sentiment value to represent a developer being satisfied as they are likely just asking an entirely new conversation. When it comes to the end of a conversation it is important to do external analysis on if the question was correctly answered which is out of the scope of our research question. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RQ3 Temporal Change in ChatGPT Interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = time_df\n",
        "\n",
        "df.columns = pd.to_datetime(df.columns)\n",
        "\n",
        "df = df.sort_index(axis=1)\n",
        "# df = df.loc[:, df.columns.year >= 2022]\n",
        "\n",
        "category_colors = {\n",
        "    'hacker news': 'red',\n",
        "    'pull request': 'blue',\n",
        "    'issue': 'green',\n",
        "    'discussion': 'orange',\n",
        "    'commit': 'purple',\n",
        "    'code file': 'brown'\n",
        "}\n",
        "\n",
        "# Plot scatter plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for category in df.index:\n",
        "    color = category_colors.get(category, 'black')  # Default to black if color not defined\n",
        "    ax.scatter(df.columns, df.loc[category], label=category, color=color)\n",
        "\n",
        "    # Add lines connecting points\n",
        "    ax.plot(df.columns, df.loc[category], linestyle='-', alpha=0.5, color=color)\n",
        "\n",
        "# Add legend and labels\n",
        "ax.legend()\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Counts')\n",
        "ax.set_title('Counts of Categories Over Time')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Temporal Analysis\n",
        "\n",
        "From conclusion 1 of RQ1, we observed that there exist changes of interest in different categories on ChatGPT. In this section, we will try to test the statisical significants of this changes.\n",
        "For this section we compute the rate of change overtime to determine the statistical significance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrame\n",
        "roc_df = df\n",
        "\n",
        "# Transpose the DataFrame\n",
        "roc_df = roc_df.transpose()\n",
        "\n",
        "# Fill NaN values with 0\n",
        "roc_df = roc_df.fillna(0)\n",
        "\n",
        "# Transpose again to compute rate of change in rows\n",
        "roc_df = roc_df.transpose()\n",
        "\n",
        "# Compute the rate of change between each month for each category\n",
        "rate_of_change = roc_df.pct_change(axis=1) * 100  # Multiply by 100 to convert to percentage change\n",
        "\n",
        "print(df)\n",
        "print(\"\\nRate of Change:\")\n",
        "print(rate_of_change)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion 1\n",
        "\n",
        "The rate of change in the discussion towards chatGPT shows that there is a massive jump in interests after the release of chatGPT. However, there also exists a sharp decrease right afterward. Though from the table we can observe that the initial increase in interest manifest overtime. The interest in discussion (hacker news, discussion, etc) is very high in the first two month, and reduced very fast after the initial hype. Where there are more pr and issue created using ChatGPT showing that although the interest in general community has decrease, many developer does attempt to implement chatGPT as part of their programmign tool kit or for problem solving. However, the commit is heavily reduced, this might suggest that user are less confident using ChatGPT to write code, or it also might because developer have using the code from chatGPT very causally without referencing the ChatGPT conversation in the commit. Therefore, the commit stats is a lot less valuable."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
